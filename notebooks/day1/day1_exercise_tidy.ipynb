{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 Exercise: Cleaning Messy Cafe Sales Data\n",
    "\n",
    "**Name:** Anton Shestakov  \n",
    "**Date:** October 8, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Transform a messy cafe sales dataset into a tidy format, designate and validate a primary key, and create summary tables.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "**File:** `../data/day1/dirty_cafe_sales.csv`  \n",
    "**Rows:** 10,000 cafe transactions  \n",
    "**Data Dictionary:** See `../data/day1/README.md`\n",
    "\n",
    "## Deliverable\n",
    "\n",
    "This notebook should **\"Restart & Run All\"** successfully when you're done!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Data Loading\n",
    "\n",
    "### TODO 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# TODO 1: Import pandas and numpy\n",
    "# Uncomment the lines below and run this cell:\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data loaded: 10,000 rows\n"
     ]
    }
   ],
   "source": [
    "# TODO 2: Load the data\n",
    "# Uncomment the lines below and run this cell:\n",
    "\n",
    "df = pd.read_csv('../data/day1/dirty_cafe_sales.csv')\n",
    "print(f\"✅ Data loaded: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2: Initial Exploration\n",
    "\n",
    "Before cleaning, let's understand what we have.\n",
    "\n",
    "### TODO 3: Display basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 10,000 rows × 8 columns\n"
     ]
    }
   ],
   "source": [
    "# TODO 3: Display the shape of the dataframe\n",
    "# Uncomment and run:\n",
    "\n",
    "print(f\"Dataset shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Item</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price Per Unit</th>\n",
       "      <th>Total Spent</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Location</th>\n",
       "      <th>Transaction Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN_1961373</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>2023-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN_4977031</td>\n",
       "      <td>Cake</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Cash</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN_4271903</td>\n",
       "      <td>Cookie</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN_7034554</td>\n",
       "      <td>Salad</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN_3160411</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Digital Wallet</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TXN_2602893</td>\n",
       "      <td>Smoothie</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-03-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TXN_4433211</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>ERROR</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>2023-10-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TXN_6699534</td>\n",
       "      <td>Sandwich</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Cash</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>2023-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TXN_4717867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>2023-07-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TXN_2064365</td>\n",
       "      <td>Sandwich</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Transaction ID      Item Quantity Price Per Unit Total Spent  \\\n",
       "0    TXN_1961373    Coffee        2            2.0         4.0   \n",
       "1    TXN_4977031      Cake        4            3.0        12.0   \n",
       "2    TXN_4271903    Cookie        4            1.0       ERROR   \n",
       "3    TXN_7034554     Salad        2            5.0        10.0   \n",
       "4    TXN_3160411    Coffee        2            2.0         4.0   \n",
       "5    TXN_2602893  Smoothie        5            4.0        20.0   \n",
       "6    TXN_4433211   UNKNOWN        3            3.0         9.0   \n",
       "7    TXN_6699534  Sandwich        4            4.0        16.0   \n",
       "8    TXN_4717867       NaN        5            3.0        15.0   \n",
       "9    TXN_2064365  Sandwich        5            4.0        20.0   \n",
       "\n",
       "   Payment Method  Location Transaction Date  \n",
       "0     Credit Card  Takeaway       2023-09-08  \n",
       "1            Cash  In-store       2023-05-16  \n",
       "2     Credit Card  In-store       2023-07-19  \n",
       "3         UNKNOWN   UNKNOWN       2023-04-27  \n",
       "4  Digital Wallet  In-store       2023-06-11  \n",
       "5     Credit Card       NaN       2023-03-31  \n",
       "6           ERROR  Takeaway       2023-10-06  \n",
       "7            Cash   UNKNOWN       2023-10-28  \n",
       "8             NaN  Takeaway       2023-07-28  \n",
       "9             NaN  In-store       2023-12-31  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO 3 (continued): Display the first 10 rows\n",
    "# Uncomment and run:\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Types:\n",
      "Transaction ID      object\n",
      "Item                object\n",
      "Quantity            object\n",
      "Price Per Unit      object\n",
      "Total Spent         object\n",
      "Payment Method      object\n",
      "Location            object\n",
      "Transaction Date    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO 3 (continued): Display column names and types\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Column Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 4: Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values (NaN) per column:\n",
      "Transaction ID         0\n",
      "Item                 333\n",
      "Quantity             138\n",
      "Price Per Unit       179\n",
      "Total Spent          173\n",
      "Payment Method      2579\n",
      "Location            3265\n",
      "Transaction Date     159\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO 4: Count missing values (NaN) in each column\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Missing Values (NaN) per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 5: Check for sentinel values\n",
    "\n",
    "Look for \"ERROR\" and \"UNKNOWN\" in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ERROR' values per column:\n",
      "Transaction ID        0\n",
      "Item                292\n",
      "Quantity            170\n",
      "Price Per Unit      190\n",
      "Total Spent         164\n",
      "Payment Method      306\n",
      "Location            358\n",
      "Transaction Date    142\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO 5: Count \"ERROR\" values in each column\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"'ERROR' values per column:\")\n",
    "print((df == 'ERROR').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'UNKNOWN' values per column:\n",
      "Transaction ID        0\n",
      "Item                344\n",
      "Quantity            171\n",
      "Price Per Unit      164\n",
      "Total Spent         165\n",
      "Payment Method      293\n",
      "Location            338\n",
      "Transaction Date    159\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO 5 (continued): Count \"UNKNOWN\" values in each column\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"'UNKNOWN' values per column:\")\n",
    "print((df == 'UNKNOWN').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: What Issues Did You Find?\n",
    "\n",
    "**TODO:** Write 2-3 sentences describing the data quality issues you observed.\n",
    "\n",
    "The data contains a big number of omissons. Depending on the variable, the share of \"bad\" values can reach almost 40% like it is for the variable \"location\". Moreover, all variables have \"object\" type as an encoding type by default that is also not true for some variables with numerical values. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Is This Data Tidy?\n",
    "\n",
    "### TODO 6: Evaluate against tidy data principles\n",
    "\n",
    "**The Three Rules:**\n",
    "1. Each variable is a column\n",
    "2. Each observation is a row\n",
    "3. Each value is a cell\n",
    "\n",
    "**Questions to answer in markdown:**\n",
    "\n",
    "1. What is the unit of observation in this dataset? (What does each row represent?)\n",
    "\n",
    "a transaction unit\n",
    "\n",
    "2. Does each variable have its own column?\n",
    "\n",
    "Yes\n",
    "\n",
    "3. Is this dataset tidy? Why or why not?\n",
    "\n",
    "According to three rules of tidy data, this data set can be called as tidy. Each variable is a separate column, each observation (which is a transaction id) is a row, and each value is a cell. Nevertheless, given into account a big number of missing values, the dataset is tidy but only structurally. In order to become a perfect version, it should be cleaned up from 'bad' values, i.e NaN, 'ERROR' and so on. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Identify and Validate Primary Key\n",
    "\n",
    "### TODO 7: Identify the primary key candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'Transaction ID' unique? True\n",
      "Total rows: 10,000\n",
      "Unique Transaction IDs: 10,000\n"
     ]
    }
   ],
   "source": [
    "# TODO 7: Check if 'Transaction ID' is unique\n",
    "# Uncomment and run:\n",
    "\n",
    "is_unique = df['Transaction ID'].is_unique\n",
    "print(f\"Is 'Transaction ID' unique? {is_unique}\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Unique Transaction IDs: {df['Transaction ID'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NULL Transaction IDs: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO 7 (continued): Check for any NULL values in 'Transaction ID'\n",
    "# Uncomment and run:\n",
    "\n",
    "null_count = df['Transaction ID'].isnull().sum()\n",
    "print(f\"NULL Transaction IDs: {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO 7 (continued): If there are duplicates, find them\n",
    "# Uncomment and run:\n",
    "\n",
    "duplicates = df[df.duplicated(subset=['Transaction ID'], keep=False)]\n",
    "print(f\"Duplicate rows: {len(duplicates)}\")\n",
    "if len(duplicates) > 0:\n",
    "     print(\"\\nShowing first few duplicates:\")\n",
    "     display(duplicates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of NaN: 0\n",
      "The number of ERROR: 0\n"
     ]
    }
   ],
   "source": [
    "nan = df['Transaction ID'].isna().sum()\n",
    "error = (df['Transaction ID'] == \"ERROR\").sum()\n",
    "\n",
    "print(f\"The number of NaN: {nan}\")\n",
    "print(f\"The number of ERROR: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 8: Write validation assertions\n",
    "\n",
    "Once you've confirmed (or fixed) the primary key, write assertions to prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transaction ID is a valid primary key\n"
     ]
    }
   ],
   "source": [
    "# TODO 8: Add assertions to validate primary key\n",
    "# Uncomment and run (these will error if checks fail):\n",
    "\n",
    "assert df['Transaction ID'].is_unique, \"❌ Duplicate transaction IDs found\"\n",
    "assert df['Transaction ID'].notna().all(), \"❌ NULL transaction IDs found\"\n",
    "assert df['Transaction ID'].isna().sum() == 0, \"❌ NA transaction IDs found\"\n",
    "assert (df['Transaction ID'] == \"ERROR\").sum() == 0, \"❌ ERROR transaction IDs found\"\n",
    "print(\"✅ Transaction ID is a valid primary key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Primary Key\n",
    "\n",
    "**TODO:** Explain what you found and any decisions you made.\n",
    "\n",
    "_[Your reflection here: Is Transaction ID a good primary key? Did you find any issues? How did you handle them?]_\n",
    "\n",
    "Based on the definition of primary key, which is basically a unique identifier, the variable Transaction ID fits to this position pretty good. Firstly, we haven't found any duplicates, in other words each row in this column is a unique value. Moreover, there haven't been NULL values, nor NaN, nor ERROR (I added a new chunk of code for checking NaN/ERROR values as well, the validation assertions block was also a bit modified).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Handle Missing Values\n",
    "\n",
    "### TODO 9: Standardize missing value representations\n",
    "\n",
    "Convert \"ERROR\", \"UNKNOWN\", and empty strings to NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Replaced 'ERROR', 'UNKNOWN', and empty strings with NaN\n"
     ]
    }
   ],
   "source": [
    "# TODO 9: Replace sentinel values with NaN\n",
    "# Uncomment and run:\n",
    "\n",
    "df = df.replace(['ERROR', 'UNKNOWN', ''], np.nan)\n",
    "print(\"✅ Replaced 'ERROR', 'UNKNOWN', and empty strings with NaN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 10: Decide how to handle missing values\n",
    "\n",
    "**Options:**\n",
    "- Drop rows with missing values in critical columns\n",
    "- Fill with default values\n",
    "- Keep as NaN (document impact on analysis)\n",
    "\n",
    "**Your strategy:**\n",
    "\n",
    "Strategy: I will drop NAN values which are contained in variables 'Item' and 'Quantity'. These two indicators are key. Without this primary information, the following transactional data deprives meaning. The analysis can exist without supplementary data on location and payment method, but when we don't know, what kind of good we have sold, then the following complementary information loses its relevance. \n",
    "\n",
    "P.S naturally we might be interested in location analysis, then we should keep this data, but assume that we are interested in transactional operations in the first place, so we have to know primary info on item and quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after standardization:\n",
      "Transaction ID         0\n",
      "Item                 969\n",
      "Quantity             479\n",
      "Price Per Unit       533\n",
      "Total Spent          502\n",
      "Payment Method      3178\n",
      "Location            3961\n",
      "Transaction Date     460\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 10,082\n"
     ]
    }
   ],
   "source": [
    "# TODO 9 (continued): Check missing values again after standardization\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Missing values after standardization:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction ID</th>\n",
       "      <th>Item</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price Per Unit</th>\n",
       "      <th>Total Spent</th>\n",
       "      <th>Payment Method</th>\n",
       "      <th>Location</th>\n",
       "      <th>Transaction Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TXN_1961373</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Takeaway</td>\n",
       "      <td>2023-09-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TXN_4977031</td>\n",
       "      <td>Cake</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Cash</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-05-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TXN_4271903</td>\n",
       "      <td>Cookie</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TXN_7034554</td>\n",
       "      <td>Salad</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-04-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TXN_3160411</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Digital Wallet</td>\n",
       "      <td>In-store</td>\n",
       "      <td>2023-06-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Transaction ID    Item Quantity Price Per Unit Total Spent  Payment Method  \\\n",
       "0    TXN_1961373  Coffee        2            2.0         4.0     Credit Card   \n",
       "1    TXN_4977031    Cake        4            3.0        12.0            Cash   \n",
       "2    TXN_4271903  Cookie        4            1.0         NaN     Credit Card   \n",
       "3    TXN_7034554   Salad        2            5.0        10.0             NaN   \n",
       "4    TXN_3160411  Coffee        2            2.0         4.0  Digital Wallet   \n",
       "\n",
       "   Location Transaction Date  \n",
       "0  Takeaway       2023-09-08  \n",
       "1  In-store       2023-05-16  \n",
       "2  In-store       2023-07-19  \n",
       "3       NaN       2023-04-27  \n",
       "4  In-store       2023-06-11  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Missing value strategy: Dropping NaN values for critical indicators: Item and Quantity\n"
     ]
    }
   ],
   "source": [
    "# TODO 10: Implement your missing value strategy\n",
    "# This is a decision point - choose your approach!\n",
    "# Below is ONE option: Keep NaN as-is (document in reflection above)\n",
    "# Uncomment and run:\n",
    "\n",
    "# For this exercise, we'll keep NaN values and handle them in analysis\n",
    "# (You could also drop rows or fill values - document your choice above!)\n",
    "print(\"✅ Missing value strategy: Dropping NaN values for critical indicators: Item and Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after standardization:\n",
      "Transaction ID         0\n",
      "Item                   0\n",
      "Quantity               0\n",
      "Price Per Unit       464\n",
      "Total Spent          432\n",
      "Payment Method      2732\n",
      "Location            3418\n",
      "Transaction Date     396\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 7,442\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['Quantity', 'Item'])\n",
    "print(\"Missing values after standardization:\")\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Fix Type Issues\n",
    "\n",
    "### TODO 11: Convert Quantity to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We don't need to run TODO 11 due to dropping all NaN in Quantity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transaction ID      object\n",
       "Item                object\n",
       "Quantity            object\n",
       "Price Per Unit      object\n",
       "Total Spent         object\n",
       "Payment Method      object\n",
       "Location            object\n",
       "Transaction Date    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 11: Convert Quantity to integer\n",
    "# Uncomment and run:\n",
    "\n",
    "#df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce').astype('Int64')\n",
    "#print(\"✅ Quantity converted to Int64 (allows NaN)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quantity'] = df['Quantity'].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 12: Convert prices to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'Price Per Unit' converted to float64\n"
     ]
    }
   ],
   "source": [
    "# TODO 12: Convert 'Price Per Unit' to float\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Price Per Unit'] = pd.to_numeric(df['Price Per Unit'], errors='coerce')\n",
    "print(\"✅ 'Price Per Unit' converted to float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 13: Convert Transaction Date to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'Total Spent' converted to float64\n"
     ]
    }
   ],
   "source": [
    "# TODO 12 (continued): Convert 'Total Spent' to float\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Total Spent'] = pd.to_numeric(df['Total Spent'], errors='coerce')\n",
    "print(\"✅ 'Total Spent' converted to float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 'Transaction Date' converted to datetime64\n"
     ]
    }
   ],
   "source": [
    "# TODO 13: Parse Transaction Date as datetime\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce')\n",
    "print(\"✅ 'Transaction Date' converted to datetime64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 14: Verify types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Column Types:\n",
      "Transaction ID              object\n",
      "Item                        object\n",
      "Quantity                     Int64\n",
      "Price Per Unit             float64\n",
      "Total Spent                float64\n",
      "Payment Method              object\n",
      "Location                    object\n",
      "Transaction Date    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# TODO 14: Display dtypes to verify conversions worked\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Updated Column Types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 15: Write type assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All types are correct!\n"
     ]
    }
   ],
   "source": [
    "# TODO 15: Add assertions to validate types\n",
    "# Uncomment and run:\n",
    "\n",
    "assert df['Quantity'].dtype in ['int64', 'Int64'], \"❌ Quantity should be integer\"\n",
    "assert df['Price Per Unit'].dtype == 'float64', \"❌ Price should be float\"\n",
    "assert df['Transaction Date'].dtype == 'datetime64[ns]', \"❌ Date should be datetime\"\n",
    "print(\"✅ All types are correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Validate Data Integrity\n",
    "\n",
    "### TODO 16: Check if Total Spent = Quantity × Price Per Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Calculated expected totals\n"
     ]
    }
   ],
   "source": [
    "# TODO 16: Calculate expected total\n",
    "# Uncomment and run:\n",
    "\n",
    "df['Calculated Total'] = df['Quantity'] * df['Price Per Unit']\n",
    "print(\"✅ Calculated expected totals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mismatches found: 0 out of 7732 rows with data\n"
     ]
    }
   ],
   "source": [
    "# TODO 16 (continued): Compare with actual Total Spent\n",
    "# This uses np.isclose() for float comparison (allows tiny rounding differences)\n",
    "# Uncomment and run:\n",
    "\n",
    "mask = df['Total Spent'].notna() & df['Calculated Total'].notna()\n",
    "mismatches = ~np.isclose(\n",
    "     df.loc[mask, 'Total Spent'], \n",
    "     df.loc[mask, 'Calculated Total'],\n",
    "     rtol=1e-05  # Relative tolerance for floating point comparison\n",
    " )\n",
    "print(f\"Mismatches found: {mismatches.sum()} out of {mask.sum()} rows with data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 17: Check for impossible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with price <= 0: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO 17: Check for negative or zero prices\n",
    "# Uncomment and run:\n",
    "\n",
    "bad_prices = df[df['Price Per Unit'] <= 0]\n",
    "print(f\"Rows with price <= 0: {len(bad_prices)}\")\n",
    "if len(bad_prices) > 0:\n",
    "     display(bad_prices[['Transaction ID', 'Item', 'Price Per Unit']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with quantity <= 0: 0\n"
     ]
    }
   ],
   "source": [
    "# TODO 17 (continued): Check for zero or negative quantities\n",
    "# Uncomment and run:\n",
    "\n",
    "bad_qty = df[df['Quantity'] <= 0]\n",
    "print(f\"Rows with quantity <= 0: {len(bad_qty)}\")\n",
    "if len(bad_qty) > 0:\n",
    "     display(bad_qty[['Transaction ID', 'Item', 'Quantity']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: Data Integrity\n",
    "\n",
    "**TODO:** What did you find? How did you handle integrity issues?\n",
    "\n",
    "We carried out the standard validation of data integrity: firstly, a new column \"Calculated Total\" was created as a product of Quantity and Price. Then we checked whether our existed column \"Total Spend\", which is actually a product of these two aforementioned variables, fits with \"Calculated Total\" and contains no values mismatches. The following steps include validation on \"bad\" values in \"Price per Unit\" and \"Quantity\" columns, whether they are positive. Ultimately, it was figured out that there were no mismatches nor errors in recorded values. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Create Summary Tables\n",
    "\n",
    "Now that data is clean, answer some business questions!\n",
    "\n",
    "### TODO 18: Total sales by payment method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8611, 9)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales by Payment Method:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Revenue</th>\n",
       "      <th>Transaction Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Credit Card</th>\n",
       "      <td>16878.0</td>\n",
       "      <td>1951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Digital Wallet</th>\n",
       "      <td>16833.0</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cash</th>\n",
       "      <td>16725.5</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Total Revenue  Transaction Count\n",
       "Payment Method                                  \n",
       "Credit Card           16878.0               1951\n",
       "Digital Wallet        16833.0               1970\n",
       "Cash                  16725.5               1958"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 18: Calculate total revenue and transaction count by payment method\n",
    "# Uncomment and run (this one is fully worked as an example):\n",
    "\n",
    "payment_summary = df.groupby('Payment Method').agg({\n",
    "     'Total Spent': 'sum',\n",
    "     'Transaction ID': 'count'\n",
    " }).round(2)\n",
    " \n",
    "payment_summary.columns = ['Total Revenue', 'Transaction Count']\n",
    "payment_summary = payment_summary.sort_values('Total Revenue', ascending=False)\n",
    " \n",
    "print(\"Sales by Payment Method:\")\n",
    "display(payment_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 19: Most popular items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Popular Items (by quantity):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Item\n",
       "Juice       3373\n",
       "Coffee      3368\n",
       "Cake        3329\n",
       "Salad       3310\n",
       "Sandwich    3245\n",
       "Smoothie    3221\n",
       "Tea         3154\n",
       "Cookie      3090\n",
       "Name: Quantity, dtype: Int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 19: Find most popular items by quantity sold\n",
    "# Pattern: df.groupby('Column')['Metric'].sum().sort_values(ascending=False)\n",
    "# Uncomment and adapt:\n",
    "\n",
    "popular_items = df.groupby('Item')['Quantity'].sum().sort_values(ascending=False)\n",
    "print(\"Most Popular Items (by quantity):\")\n",
    "display(popular_items.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Revenue Items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Item\n",
       "Salad       15810.0\n",
       "Sandwich    12220.0\n",
       "Smoothie    12096.0\n",
       "Juice        9588.0\n",
       "Cake         9516.0\n",
       "Coffee       6448.0\n",
       "Tea          4506.0\n",
       "Cookie       2928.0\n",
       "Name: Total Spent, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 19 (continued): Find highest revenue items\n",
    "# Use the same pattern but with 'Total Spent' instead of 'Quantity'\n",
    "# Uncomment and adapt:\n",
    "\n",
    "revenue_items = df.groupby('Item')['Total Spent'].sum().sort_values(ascending=False).round(2)\n",
    "print(\"Highest Revenue Items:\")\n",
    "display(revenue_items.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 20: Location comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sales by Location:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transaction Count</th>\n",
       "      <th>Total Revenue</th>\n",
       "      <th>Avg Transaction Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>In-store</th>\n",
       "      <td>2602</td>\n",
       "      <td>22360.0</td>\n",
       "      <td>9.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Takeaway</th>\n",
       "      <td>2591</td>\n",
       "      <td>21697.0</td>\n",
       "      <td>8.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Transaction Count  Total Revenue  Avg Transaction Value\n",
       "Location                                                         \n",
       "In-store               2602        22360.0                   9.05\n",
       "Takeaway               2591        21697.0                   8.79"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO 20: Compare transaction volume and average transaction value by location\n",
    "# This uses .agg() with multiple functions (like TODO 18)\n",
    "# Uncomment and run:\n",
    "\n",
    "location_summary = df.groupby('Location').agg({\n",
    "     'Transaction ID': 'count',\n",
    "     'Total Spent': ['sum', 'mean']\n",
    " }).round(2)\n",
    " \n",
    "location_summary.columns = ['Transaction Count', 'Total Revenue', 'Avg Transaction Value']\n",
    "print(\"Sales by Location:\")\n",
    "display(location_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Final Validation\n",
    "\n",
    "### TODO 21: Run all validations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final validation...\n",
      "\n",
      "✅ Primary key validated\n",
      "✅ Types validated\n",
      "✅ Data ranges validated\n",
      "\n",
      "✅ All validations passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO 21: Gather all your assertions in one cell to prove data quality\n",
    "# Uncomment and run:\n",
    "\n",
    "print(\"Running final validation...\\n\")\n",
    "# \n",
    "# # Primary key\n",
    "assert df['Transaction ID'].is_unique, \"❌ Duplicate transaction IDs\"\n",
    "assert df['Transaction ID'].notna().all(), \"❌ NULL transaction IDs\"\n",
    "print(\"✅ Primary key validated\")\n",
    "# \n",
    "# # Types\n",
    "assert df['Quantity'].dtype in ['int64', 'Int64'], \"❌ Quantity type wrong\"\n",
    "assert df['Price Per Unit'].dtype == 'float64', \"❌ Price type wrong\"\n",
    "assert df['Transaction Date'].dtype == 'datetime64[ns]', \"❌ Date type wrong\"\n",
    "print(\"✅ Types validated\")\n",
    "# \n",
    "# # Data ranges (only check non-null values)\n",
    "assert (df['Quantity'].dropna() > 0).all(), \"❌ Invalid quantities found\"\n",
    "assert (df['Price Per Unit'].dropna() > 0).all(), \"❌ Invalid prices found\"\n",
    "print(\"✅ Data ranges validated\")\n",
    "# \n",
    "print(\"\\n✅ All validations passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 10: Documentation\n",
    "\n",
    "### TODO 22: Document your data cleaning process\n",
    "\n",
    "Write a brief summary (8-10 sentences) of:\n",
    "1. What problems you found\n",
    "2. What decisions you made\n",
    "3. What the implications are for analysis\n",
    "4. What a stakeholder should know about this data\n",
    "\n",
    "---\n",
    "\n",
    "## Data Cleaning Summary\n",
    "\n",
    "The dataset was originally messy, with inconsistent missing value encodings (NaN, ERROR, NULL) and all variables were encoded as object type. We standardized missing values to a single NaN format and converted variables into appropriate data types (i.e numerical for Quantity and Price). Since Item and Quantity are critical for transactional analysis, rows with missing values in these columns were dropped, reducing the dataset to 8,166 observations.\n",
    "A primary key based on Transaction ID was validated, ensuring uniqueness and consistency for potential combining with other datasets. Assertions confirmed that calculated product between quantity and price matched reported Total Spent. Although the data is now structurally tidy and validated, some variables such as Location and Payment Method still contain high rates of missing values. This means analyses relying on those categories may be less representative, while analyses focusing on items and quantities can be performed at full quality. Stakeholders should know that the dataset is reliable for product level sales insights but limited for geographic or payment-related breakdowns. To sum it up, the data is sufficiently clean for core sales and revenue analysis, with clear limitations documented for context.\n",
    "\n",
    "### Issues Found\n",
    "- A large number of missing values in complementary variables such that Location (Around 40%)\n",
    "- Omissions in critical indicators in columns Item/Quantity\n",
    "- Different names of missing values including 'NaN', 'ERROR', 'NULL'\n",
    "- Incorrect data encoding: all values were object type\n",
    "\n",
    "### Actions Taken\n",
    "- Converting all missing values to a single standardized form NaN\n",
    "- Dropping NaN values in columns with critical information: Item and Quantity. That, in turn, reduced our dataset from 10,000 observation to 8,611\n",
    "- Encoding modification: converting numerical variables to numerical type for the following analysis\n",
    "\n",
    "### Assumptions Made\n",
    "- The dataset and following analysis emphasizes the transactional data which must include non-null values of item and its sold quantity. For this reason we dropped NaN values in these columns, reducing the total number of dataset observation\n",
    "\n",
    "### Implications for Analysis\n",
    "- There is still a high rate of missing values in some categories, such as Location or Payment Method\n",
    "- Data has a tidy structure\n",
    "- There are no imputations, but dropping the missing values out of critical categories: Item and Quantity\n",
    "- Validation checks passed\n",
    "- No error in computational column Total Spend according to assertion data validation \n",
    "\n",
    "### Data Quality Assessment\n",
    "- The data is mostly cleaned, but contains a significant number of missing values in some categories. We got the following results on missing values after standardization:\n",
    "Transaction ID         0\n",
    "Item                   0\n",
    "Quantity               0\n",
    "Price Per Unit       464\n",
    "Total Spent          432\n",
    "Payment Method      2732\n",
    "Location            3418\n",
    "Transaction Date     396\n",
    "\n",
    "That means we could evaluate the quality of this data from 60,30% to 100% depending on the type of analysis (give into account that we have 8,166 obs after all modifciations). For example, if the analysis requires the information and analysis only on quantity by items, then the data is fully ready for performing a high-quality analysis. Including the column Price Per Unit will deteriorate the representativeness of the analysis from 100% to 94,3% because of missing values. \n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've successfully cleaned a real messy dataset using tidy data principles!\n",
    "\n",
    "**Final check:** Can you **\"Restart & Run All\"** successfully? That's the gold standard!\n",
    "\n",
    "**Reflection:** What was the hardest part? What did you learn?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
